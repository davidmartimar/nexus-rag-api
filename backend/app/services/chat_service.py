import os
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory

# Configuration
CHROMA_DB_DIR = "/app/chroma_db"
from app.services.rag_service import DEFAULT_COLLECTION_NAME

def get_answer(query: str, collection_name: str = DEFAULT_COLLECTION_NAME, history: list = []):
    """
    1. Embeds the query.
    2. Searches ChromaDB for relevant chunks.
    3. Sends chunks + query + history to LLM.
    4. Returns answer + sources.
    """
    try:
        # 1. Initialize Vector DB Connection
        embeddings = OpenAIEmbeddings()
        vector_db = Chroma(
            persist_directory=CHROMA_DB_DIR,
            embedding_function=embeddings,
            collection_name=collection_name
        )

        # 2. Initialize LLM (The Brain)
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        
        # 3. Initialize Memory
        # k=5 means looking back at the last 5 exchanges
        memory = ConversationBufferWindowMemory(
            memory_key="chat_history", 
            return_messages=True, 
            k=5,
            output_key="answer"
        )
        
        # Reconstruct Memory from History
        for exchange in history:
            if "user" in exchange and "assistant" in exchange:
                memory.save_context(
                    {"input": exchange["user"]}, 
                    {"answer": exchange["assistant"]}
                )

        # 4. Create Conversational RAG Chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vector_db.as_retriever(search_kwargs={"k": 6}),
            memory=memory,
            return_source_documents=True,
            # We want to return the answer generated by the LLM
            output_key="answer"
        )

        # 5. Ask the question
        result = qa_chain({"question": query})
        
        # 6. Format Output
        answer = result["answer"]
        sources = [doc.page_content for doc in result["source_documents"]]
        
        return {
            "answer": answer,
            "sources": sources
        }

    except Exception as e:
        print(f"Error generating answer: {e}")
        raise e